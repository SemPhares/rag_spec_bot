# GLOBAL CONFIG
CHUNCK_SIZE = 1024
CHUNK_OVERLAP = 80
NUM_GPU = 1

# MODEL CONFIG
PHI3_MODEL_NAME = "phi3"
PHI3_INSTRUCT_REPO_ID = "bartowski/Phi-3-mini-4k-instruct-v0.3-GGUF"
PHI3_INSTRUCT_FILENAME = "*v0.3-Q3_K_L.gguf"

LLAMA3_MODEL_NAME = "llama3"
LLAMA3_INSTRUCT_REPO_ID = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
LLAMA3_INSTRUCT_FILENAME = "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
LLAMA3_MODEL_PATH = "app/specbot/model_api/models_w/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"

IMAGE_MODEL_NAME = "LLaVAv1.5-7b"
IMAGE_MODEL_REPO_ID = "mys/ggml_llava-v1.5-7b"
IMAGE_MODEL_FILENAME = "ggml-model-q4_k.gguf"
IMAGE_MODEL_PATH = "app/specbot/model_api/models_w/mmproj-model-f16.gguf"

# GEMINI
GEMINI_API_ENDPOINT = "https://api.gemini.com/v1/text"

# BASE MODELS TO USE
OLLAMA_BASE_MODEL_NAME = "llama3"
LLAMA_CPP_BASE_MODEL_NAME = "llama3"
LLAMA_CPP_BASE_MODEL_REPO_ID = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
LLAMA_CPP_BASE_MODEL_FILENAME = "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
LLAMA_CPP_BASE_MODEL_PATH = "app/specbot/model_api/models_w/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"

# EMBEDDING MODELS
EMBEDDING_MODEL_PATH = "app/specbot/model_api/models_w/capybarahermes-2.5-mistral-7b.Q4_K_M.gguf"
EMBEDDING_MODEL_NAME = "Mistral-7b"
EMBEDDING_MODEL_REPO_ID = "TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF"
EMBEDDING_MODEL_FILENAME = "capybarahermes-2.5-mistral-7b.Q4_K_M.gguf"