# GLOBAL CONFIG
CHUNCK_SIZE = 1024
CHUNK_OVERLAP = 80
NUM_GPU = -1
IMAGES_EXTENSIONS = 'jpg', 'jpeg', 'png', 'gif' 
ACCEPTED_EXTENSION = 'docx', 'doc', 'pdf', 'txt', 'xls', 'xlsx'

# MODEL CONFIG
MISTRAL_7B_MODEL_NAME = "mistral:7b"
MISTRAL_7B_REPO_ID = "TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF"
MISTRAL_7B_PATH = "specbot/model_api/models_w/capybarahermes-2.5-mistral-7b.Q4_K_M.gguf"

LLAMA3_MODEL_NAME = "llama3"
LLAMA3_INSTRUCT_REPO_ID = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
LLAMA3_INSTRUCT_FILENAME = "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
LLAMA3_MODEL_PATH = "specbot/model_api/models_w/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"

IMAGE_MODEL_NAME = "llava:7b"
IMAGE_MODEL_REPO_ID = "mys/ggml_llava-v1.5-7b"
IMAGE_MODEL_FILENAME = "ggml-model-q4_k.gguf"
IMAGE_MODEL_PATH = "specbot/model_api/models_w/mmproj-model-f16.gguf"

# GEMINI
GEMINI_API_ENDPOINT = "https://api.gemini.com/v1/text"

# BASE MODELS TO USE
OLLAMA_BASE_MODEL_NAME = "llama3"
LLAMA_CPP_BASE_MODEL_NAME = "llama3"
LLAMA_CPP_BASE_MODEL_REPO_ID = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
LLAMA_CPP_BASE_MODEL_FILENAME = "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
LLAMA_CPP_BASE_MODEL_PATH = "specbot/model_api/models_w/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"

# EMBEDDING MODELS
EMBEDDING_MODEL_PATH = "specbot/model_api/models_w/capybarahermes-2.5-mistral-7b.Q4_K_M.gguf"
EMBEDDING_MODEL_NAME = "mistral:7b"
EMBEDDING_MODEL_REPO_ID = "TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF"
EMBEDDING_MODEL_FILENAME = "capybarahermes-2.5-mistral-7b.Q4_K_M.gguf"